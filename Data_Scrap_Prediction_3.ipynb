{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "executionInfo": {
     "elapsed": 481,
     "status": "ok",
     "timestamp": 1680581336946,
     "user": {
      "displayName": "Alan TONG",
      "userId": "00717208569138961469"
     },
     "user_tz": -480
    },
    "id": "AG0armhriaVZ",
    "outputId": "68289992-d48d-4924-ff24-3ae606c73f03"
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "reddit = praw.Reddit(client_id='k26fEW5kz3fD6lx32iRqDQ', client_secret='yVGUF6DKD44M8_UR9rYyjZ98qR_F-Q', user_agent='Scraper')\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "posts = []\n",
    "subreddit = reddit.subreddit('finance')\n",
    "for post in subreddit.top(time_filter=\"month\",limit=1000):\n",
    "    posts.append([post.title, post.created])\n",
    "    \n",
    "posts = pd.DataFrame(posts,columns=['title', 'created'])\n",
    "\n",
    "posts['created'] = ( pd.to_datetime(posts['created'],unit='s').dt.tz_localize('utc').dt.tz_convert('America/New_York'))\n",
    "posts = posts.sort_values('created')\n",
    "\n",
    "outdir = \"./data/news\"\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "    \n",
    "posts.to_csv('./data/news/news3.csv', encoding = 'utf-8-sig') \n",
    "posts = pd.DataFrame(posts, columns = ['tiitle', 'created'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-f9b3aaf51616>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"created\"][i] = df[\"created\"][i][:10]\n"
     ]
    }
   ],
   "source": [
    "df_input = pd.read_csv(\"./data/news/input_news.csv\")\n",
    "df_input = df_input.rename(columns={'date': 'Date', 'input': 'News Header'})\n",
    "df_input.set_index(\"Date\", inplace=True)\n",
    "df = pd.read_csv(\"./data/news/news3.csv\")\n",
    "df = df.iloc[::-1]\n",
    "for i in range(len(df)):\n",
    "    df[\"created\"][i] = df[\"created\"][i][:10]\n",
    "df = df.rename(columns={'created': 'Date', 'title': 'News Header'})\n",
    "df.set_index(\"Date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\COMP5214\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "df.sort_index(ascending=True, inplace=True)\n",
    "df['Date'] = df.index\n",
    "for i in range(len(df.iloc[:,0])):\n",
    "    if (df['Date'].iloc[i][5] == \"1\"):\n",
    "        df['Date'].iloc[i] = df['Date'].iloc[i][-2:]+\"/\"+df['Date'].iloc[i][5:7]+\"/\"+df['Date'].iloc[i][0:4]\n",
    "    else :\n",
    "        df['Date'].iloc[i] = df['Date'].iloc[i][-2:]+\"/\"+df['Date'].iloc[i][6]+\"/\"+df['Date'].iloc[i][0:4]\n",
    "    if (df['Date'].iloc[i][0] == \"0\"):\n",
    "        df['Date'].iloc[i] = df['Date'].iloc[i][1:]\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "        df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "df = df.append(df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cryptos_df = []\n",
    "cryptos = [\"BTC\", \"ADA\", \"BCH\", \"BNB\", \"DOGE\", \"ETH\", \"FTT\", \"LINK\", \"OKB\", \"SOL\"]\n",
    "\n",
    "for i in range(len(cryptos)):\n",
    "    string = \"./data/processed/\" + cryptos[i]+\"_CNN_predicted.csv\"\n",
    "    crypto_df = pd.read_csv(string)\n",
    "    crypto_df = crypto_df.rename(columns={'Unnamed: 0.1': 'Date'})\n",
    "    crypto_df.set_index('Date', inplace=True)\n",
    "    cryptos_df.append(crypto_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RNN model\n",
    "class Text_RNN(nn.Module):\n",
    "    def __init__(self, n_vocab, embedding_dim, n_hidden, n_layers, dropout):\n",
    "        super(Text_RNN, self).__init__()\n",
    "        self.emb = nn.Embedding(n_vocab, embedding_dim) \n",
    "        self.rnn = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=n_hidden,\n",
    "                num_layers=n_layers,\n",
    "                dropout=dropout, \n",
    "                batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(n_hidden, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, sent, sent_len):\n",
    "        # sent: batch_size, max_sent_len\n",
    "        # sent_len: batch_size\n",
    "        sent_emb = self.emb(sent)  #batch_size, max_sent_len, embedding_dim\n",
    "        outputs, h_n = self.rnn(sent_emb)\n",
    "        outputs = self.dropout(h_n)\n",
    "        outputs = self.fc(outputs)\n",
    "        out = self.sigmoid(outputs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define saving and loading of models\n",
    "def save_checkpoint(save_path, model, optimizer, val_acc):\n",
    "    if save_path==None:\n",
    "        return\n",
    "    save_path = save_path \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'val_acc': val_acc}\n",
    "\n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to {save_path}')\n",
    "\n",
    "def load_checkpoint(save_path, model, optimizer):\n",
    "    save_path = save_path \n",
    "    state_dict = torch.load(save_path)\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    val_acc = state_dict['val_acc']\n",
    "    print(f'Model loaded from {save_path}')\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "class Text_RNN_m2(nn.Module):\n",
    "    def __init__(self, n_vocab, embedding_dim, n_hidden, n_layers, dropout):\n",
    "        super(Text_RNN_m2, self).__init__()\n",
    "        self.emb = nn.Embedding(n_vocab, embedding_dim) \n",
    "        self.lstm = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=n_hidden,\n",
    "                num_layers=n_layers,\n",
    "                dropout=dropout, \n",
    "                batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(n_hidden, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, sent, sent_len):\n",
    "        sent_emb = self.emb(sent) \n",
    "        outputs, (h_n, c_n) = self.lstm(sent_emb)\n",
    "        outputs = self.dropout(h_n)\n",
    "        outputs = self.fc(outputs)\n",
    "        out = self.sigmoid(outputs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TEST_non_ensemble(model, test_loader, device): \n",
    "    model.to(device)\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            sent, sent_len = batch.news\n",
    "            inputs_sent = sent.to(device)\n",
    "            inputs_sent_len = sent_len.to(device)\n",
    "            # Make Prediction\n",
    "            outputs = model(inputs_sent, inputs_sent_len)\n",
    "            outputs = outputs.reshape(-1,1)\n",
    "            for predicted in outputs:\n",
    "                y_pred.append(predicted.item())\n",
    "    return y_pred\n",
    "\n",
    "def TEST_ensemble(model_1_tuple, model_2_tuple, model_3_tuple, test_loader, device, average_cal): \n",
    "    model_1 = model_1_tuple[0].to(device)\n",
    "    model_2 = model_2_tuple[0].to(device)\n",
    "    model_3 = model_3_tuple[0].to(device)\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        model_1.eval()\n",
    "        model_2.eval()\n",
    "        model_3.eval()\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            sent, sent_len = batch.news\n",
    "            inputs_sent = sent.to(device)\n",
    "            inputs_sent_len = sent_len.to(device)\n",
    "            \n",
    "            # Make Prediction\n",
    "            outputs_1 = model_1(inputs_sent, inputs_sent_len)\n",
    "            outputs_1 = outputs_1.reshape(-1,1)\n",
    "            outputs_2 = model_2(inputs_sent, inputs_sent_len)\n",
    "            outputs_2 = outputs_2.reshape(-1,1)\n",
    "            outputs_3 = model_3(inputs_sent, inputs_sent_len)\n",
    "            outputs_3 = outputs_3.reshape(-1,1)\n",
    "\n",
    "            if average_cal == \"Simple average\":\n",
    "                outputs = (outputs_1 + outputs_2 +outputs_3) / float(3.0)\n",
    "            elif average_cal == \"Weighted average\":\n",
    "                w1, w2, w3 = weighted_average(model_1_tuple, model_2_tuple, model_3_tuple)\n",
    "                outputs = outputs_1*w1 + outputs_2*w2 + outputs_3*w3\n",
    "            for predicted in outputs:\n",
    "                y_pred.append(predicted.item())\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-a3f96757b401>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['Adj Close'] = test_df['Adj Close'].astype('int')\n",
      "C:\\Users\\user\\anaconda3\\envs\\COMP5214\\lib\\site-packages\\torchtext\\data\\field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "C:\\Users\\user\\anaconda3\\envs\\COMP5214\\lib\\site-packages\\torchtext\\data\\example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "C:\\Users\\user\\anaconda3\\envs\\COMP5214\\lib\\site-packages\\torchtext\\data\\example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "C:\\Users\\user\\anaconda3\\envs\\COMP5214\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./trained_parameters/LSTM model\n",
      "Model loaded from ./trained_parameters/LSTM model\n",
      "Model loaded from ./trained_parameters/LSTM model\n",
      "Model loaded from ./trained_parameters/LSTM model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\COMP5214\\lib\\site-packages\\torchtext\\data\\iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "C:\\Users\\user\\anaconda3\\envs\\COMP5214\\lib\\site-packages\\torchtext\\data\\batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "<ipython-input-10-a3f96757b401>:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['RNN_Prediction'] = predicted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./trained_parameters/LSTM model\n",
      "Model loaded from ./trained_parameters/LSTM model\n",
      "Model loaded from ./trained_parameters/LSTM model\n",
      "Model loaded from ./trained_parameters/LSTM model\n",
      "Model loaded from ./trained_parameters/LSTM model\n",
      "Model loaded from ./trained_parameters/LSTM model\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "for i in range(len(cryptos)):\n",
    "    df_pred = cryptos_df[i][[\"Adj Close\"]]\n",
    "    df_merge = pd.merge(df, df_pred, how=\"inner\", on=[\"Date\"])\n",
    "\n",
    "    if 'Unnamed: 0' in  df_merge.columns:\n",
    "        df_merge.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "    prediction_column = df_merge[['Adj Close', 'News Header']]\n",
    "    prediction_row = prediction_column.iloc[-1:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    test_df = prediction_row\n",
    "    test_df['Adj Close'] = test_df['Adj Close'].astype('int')\n",
    "    # The label field is redundant since it will be predicted. Just for compatitablity\n",
    "    test_df.rename(columns={\"Adj Close\": \"label\", \"News Header\": \"news\"})\n",
    "\n",
    "    PATH = \"./data/news/\"\n",
    "    test_df.to_csv(PATH + \"news_prediction.csv\", index=False)\n",
    "\n",
    "    txt_field = Field(tokenize=word_tokenize, lower=True, batch_first=True, include_lengths=True) \n",
    "    label_field = Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "\n",
    "    train = TabularDataset(path=PATH + 'training.csv', format='csv', fields=[('label', label_field), ('news', txt_field)], skip_header=True)\n",
    "    test = TabularDataset(path=PATH + 'news_prediction.csv', format='csv', fields=[('label', label_field), ('news', txt_field)], skip_header=True)\n",
    "    txt_field.build_vocab(train, min_freq=2)\n",
    "\n",
    "    # Build into the vocabulary from distinct words\n",
    "    vocab_distinct = set(txt_field.vocab.itos)\n",
    "\n",
    "    # Make a new object instance for LSTM model\n",
    "    model_2 = Text_RNN_m2(n_vocab=len(vocab_distinct), embedding_dim=50, n_hidden=64, n_layers=1, dropout=0.1).cuda()\n",
    "\n",
    "    save_name_model_2 = 'LSTM model'\n",
    "    path = \"./trained_parameters/\" + save_name_model_2\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = Adam(model_2.parameters())\n",
    "    load_checkpoint(path, model_2, optimizer)\n",
    "\n",
    "    test_iter = BucketIterator(test, batch_size=1, shuffle=False)\n",
    "    \n",
    "    device = 'cpu'\n",
    "    best_model = model_2\n",
    "    predicted = TEST_non_ensemble(best_model, test_iter, device)\n",
    "    test_df['RNN_Prediction'] = predicted\n",
    "    test_df = test_df[['RNN_Prediction']]\n",
    "    \n",
    "    \n",
    "    df_output = pd.merge(test_df, cryptos_df[i], how=\"outer\", on=[\"Date\"])\n",
    "    if 'Unnamed: 0' in  df_output.columns:\n",
    "        df_output.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "    if 'Unnamed: 0.1' in  df_output.columns:\n",
    "        df_output.drop(columns=[\"Unnamed: 0.1\"], inplace=True)\n",
    "    df_output = df_output.iloc[1:,].append(df_output.iloc[0,:])\n",
    "    o_string = \"./data/processed/\" + cryptos[i]+\"_RNN_CNN_predicted.csv\"\n",
    "    df_output.to_csv(o_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOayOuuOvK8fkFyiG1EB1IG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
